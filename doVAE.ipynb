{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33ce4f97-282d-4828-b95c-cdb6141228aa",
   "metadata": {},
   "source": [
    "# VAE with do notation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f104fd10-58df-40de-8663-c811ce962aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "from abc import abstractmethod\n",
    "from torch.nn import functional as F\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1c61e2e-b27d-4a78-b747-bbba06717e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a6855cd-dd48-427e-a9b1-012527441128",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.init as init\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms as T\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c3a4d06-7c97-4b49-ad33-a3e0b82371d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0,\"/cmlscratch/margot98/Causal_Disentangle/3dshape_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "731b67d1-bcf7-4d63-9b5f-4d7119932633",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import ShapeDataset,CelebaDataset\n",
    "from dataset_3d_shape import sample_3dshape_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e132c44b-fbd7-4385-b756-6213004b3231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_path = \"/cmlscratch/margot98/Causal_Disentangle/3dshape_data\"\n",
    "# attr_path =  \"/cmlscratch/margot98/Causal_Disentangle/3dshape_data/sample_source.csv\"\n",
    "\n",
    "data_path = \"/fs/cml-datasets/CelebA/Img/img_align_celeba\"\n",
    "attr_path =  \"/fs/cml-datasets/CelebA/Anno/list_attr_celeba.txt\"\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "72419b86-7f40-4ac6-8167-39d4b7fa8318",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 64\n",
    "transform = T.Compose([\n",
    "    T.Resize([image_size, image_size]),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# train_dataset = ShapeDataset( data_path='/cmlscratch/margot98/Causal_Disentangle/3dshape_data', \n",
    "#                               attr_path='/cmlscratch/margot98/Causal_Disentangle/3dshape_data/sample_source.csv',\n",
    "#                               attr=['floor_hue', 'shape'],\n",
    "#                               transform=transform\n",
    "#                              )\n",
    "train_dataset = train_dataset = CelebaDataset(data_path,attr_path,\n",
    "                              attr=['Eyeglasses'],\n",
    "                              transform=transform)\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=1024,\n",
    "                          shuffle=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "54204f5e-0078-457e-af35-884910686fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kaiming_init(m):\n",
    "    if isinstance(m, (nn.Linear, nn.Conv2d)):\n",
    "        init.kaiming_normal_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.fill_(0)\n",
    "    elif isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d)):\n",
    "        m.weight.data.fill_(1)\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.fill_(0)\n",
    "            \n",
    "class View(nn.Module):\n",
    "    def __init__(self, size):\n",
    "        super(View, self).__init__()\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, tensor):\n",
    "        return tensor.view(self.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e3befec1-1522-40fe-aec5-5f79679e008b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class doVAE(nn.Module):\n",
    "\n",
    "    def __init__(self,z_dim, c_dim = 2, in_channels=3):\n",
    "        super(doVAE, self).__init__()\n",
    "        \n",
    "        self.z_dim = z_dim\n",
    "        self.c_dim = c_dim\n",
    "        # encoder output dim: c_Num*z_dim for mu_z, c_Num*z_dim for sigma_z, c_Num for mu_pi, c_Num for sigma_mu\n",
    "        self.model_encoder= nn.Sequential(\n",
    "            # B,  32, 32, 32\n",
    "            nn.Conv2d(in_channels, out_channels=32, kernel_size=4, stride=2, padding=1),          \n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            # B,  32, 16, 16\n",
    "            nn.Conv2d(32, out_channels=32, kernel_size=4, stride=2, padding=1),  \n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            # B,  64,  8,  8\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1),  \n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            # B,  64,  4,  4\n",
    "            nn.Conv2d(64, 64, kernel_size=4, stride=2, padding=1),          \n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            # B, 256,  1,  1\n",
    "            nn.Conv2d(64, 256, kernel_size=4, stride=1),           \n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            View((-1, 256*1*1)),                 # B, 256\n",
    "            \n",
    "            # for mu and logvar\n",
    "            nn.Linear(256,  c_dim*(2*z_dim+2)),      \n",
    "        )\n",
    "        \n",
    "            \n",
    "        self.model_decoder = nn.Sequential(\n",
    "            nn.Linear(z_dim, 256),               # B, 256\n",
    "            View((-1, 256, 1, 1)),               # B, 256,  1,  1\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(256, 64, 4),      # B,  64,  4,  4\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 64, 4, 2, 1), # B,  64,  8,  8\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, 4, 2, 1), # B,  32, 16, 16\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 32, 4, 2, 1), # B,  32, 32, 32\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, in_channels, 4, 2, 1),  # B, 3, 64, 64\n",
    "        )\n",
    "        \n",
    "\n",
    "        self.weight_init()\n",
    "        \n",
    "    def weight_init(self):\n",
    "        for block in self._modules:\n",
    "            for m in self._modules[block]:\n",
    "                kaiming_init(m)\n",
    "        \n",
    "                \n",
    "    def encode(self, X):\n",
    "        \n",
    "        result = self.model_encoder(X)\n",
    "        X_len = X.shape[0]\n",
    "        mu = torch.zeros(X_len,self.c_dim * self.z_dim)\n",
    "        logvar = torch.zeros(X_len,self.c_dim * self.z_dim)\n",
    "        mu_pi = torch.zeros(X_len,self.c_dim)\n",
    "        var_pi = torch.zeros(X_len,self.c_dim)\n",
    "        for c in range(self.c_dim):\n",
    "            mu[:,c*self.z_dim:(c+1)*self.z_dim] = result[:,c*self.z_dim : (c+1)*self.z_dim]\n",
    "            logvar[:,c*self.z_dim:(c+1)*self.z_dim] = result[:,(self.c_dim+c)*self.z_dim : (self.c_dim+c+1)*self.z_dim]\n",
    "            mu_pi[:,c] = result[:,2*self.c_dim*self.z_dim + c]\n",
    "            var_pi[:,c] = result[:,2*self.c_dim*self.z_dim + self.c_dim + c]\n",
    "                                \n",
    "        \n",
    "        return mu, logvar, mu_pi, var_pi\n",
    "                                 \n",
    "    \n",
    "    def decode(self, z):\n",
    "        \n",
    "        return self.model_decoder(z)\n",
    "    \n",
    "    \n",
    "    def generate(self, num_samples, current_device):                         \n",
    "        z = torch.randn(num_samples, self.z_dim)\n",
    "        z = z.to(current_device)\n",
    "        samples = self.decode(z)\n",
    "        return samples\n",
    "    \n",
    "\n",
    "    def reparameterize(self, mu, logvar,mu_pi,var_pi):\n",
    "        X_len = mu.shape[0]\n",
    "        z_all = torch.zeros(X_len, self.z_dim)\n",
    "        Pi = torch.zeros(X_len, self.c_dim)\n",
    "        for c in range(self.c_dim):\n",
    "            std = torch.exp(0.5 * logvar[:,c*self.z_dim:(c+1)*self.z_dim])\n",
    "            eps = torch.randn_like(std)\n",
    "            curr_z = eps * std + mu[:,c*self.z_dim:(c+1)*self.z_dim]\n",
    "            \n",
    "            std_pi = torch.exp(0.5 * var_pi[:,c])\n",
    "            eps_pi = torch.randn_like(std_pi)\n",
    "            curr_pi = eps_pi * std_pi + mu_pi[:,c]\n",
    "            \n",
    "            z_all += curr_z * curr_pi[:,None]\n",
    "            Pi[:,c] = curr_pi\n",
    "        \n",
    "        return z_all, Pi\n",
    "    \n",
    "    \n",
    "\n",
    "    def forward(self, X):\n",
    "        mu, logvar, mu_pi, var_pi = self.encode(X)\n",
    "        z,Pi = self.reparameterize(mu, logvar, mu_pi, var_pi)\n",
    "        reconstructed_z = self.decode(z)\n",
    "        return reconstructed_z, mu, logvar, Pi\n",
    "    \n",
    "    def calculate_loss(self, X, reconstructed_z, mu, logvar, Pi, label, pi_weight = 1.0, kl_weight = 0.5):\n",
    "        \"\"\"\n",
    "        Loss term currently contains: reconstruction loss, classification loss for p(c|x)\n",
    "        \"\"\"\n",
    "        # classification loss:\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        pi_loss = criterion(Pi, label.float())\n",
    "        \n",
    "        # reconstruction loss:\n",
    "        rec_loss = F.mse_loss(reconstructed_z, X)\n",
    "        \n",
    "        kld_loss = 0\n",
    "        for c in range(self.c_dim):\n",
    "            # regulate to be N(mu_c, I), where mu_c learnt from NN\n",
    "            log_var_c = logvar[:,c*self.z_dim:(c+1)*self.z_dim]\n",
    "            kld_loss += torch.mean(-0.5 * torch.sum(1 + log_var_c - log_var_c.exp(), dim = 1), dim = 0)\n",
    "        \n",
    "        loss = rec_loss + pi_weight * pi_loss + kl_weight * kld_loss\n",
    "        \n",
    "        return loss, rec_loss, pi_loss, kld_loss\n",
    "        \n",
    "    \n",
    "#     def calculate_loss(self, x1, x2):\n",
    "#         \"\"\"\n",
    "#         Here we compute the distance d(E[Z_i|do(Z_{-i}), X], E[Z_i|X])\n",
    "#         E[Z_i | X] = \\sum_{i}f(x_i) /N = \\sum_{c} P(C = c)E[Z_i|X, C = c]\n",
    "#         E[Z_i | do(Z_{-i}), X] = \\sum_{c}P(C = c)E[Z_i|Z_{-i},X, C = c]\n",
    "        \n",
    "#         Computation:\n",
    "#         sum_c P(C = c)\\sum_i d(E[Z_i|do(Z_{-i}), X, C = c], E[Z_i|X, C = c])\n",
    "#         \"\"\"\n",
    "#         prob_c = []\n",
    "#         prob_c.append(len(x1)/(len(x1)+len(x2)))\n",
    "#         prob_c.append(len(x2)/(len(x1)+len(x2)))\n",
    "        \n",
    "#         output_list, z_sample_list, z_mean_list, z_logvar_list = self.forward(x1, x2)\n",
    "        \n",
    "#         Dist = 0\n",
    "#         for j in range(self.c_Num):\n",
    "#             mu_j = z_mean_list[j]\n",
    "#             logvar = z_logvar_list[j]\n",
    "#             z_sample =  z_sample_list[j]\n",
    "#             dist_j = 0\n",
    "#             diff = mu_j - z_sample\n",
    "#             length = len(mu_j)\n",
    "#             for i in range(self.z_dim):\n",
    "#                 E = mu_j[:,i]\n",
    "#                 logsig_i_minus_i = torch.cat((logvar[:,i,:i].T, logvar[:,i,i+1:].T)).T\n",
    "#                 logsig_minus_minus = torch.cat((logvar[:,:i,:i].reshape(length,i*i), \n",
    "#                                                 logvar[:,:i,i+1:].reshape(length,i*(self.z_dim-i-1)),\n",
    "#                                                 logvar[:,i+1:,:i].reshape(length,i*(self.z_dim-i-1)),\n",
    "#                                                 logvar[:,i+1:,i+1:].reshape(length,(self.z_dim-i-1)*(self.z_dim-i-1))), \n",
    "#                                                 axis = 1).reshape(-1,self.z_dim-1, self.z_dim-1)\n",
    "                \n",
    "#                 diff_minus = torch.cat((diff[:,:i].T, diff[:,i+1:].T)).T\n",
    "#                 do_E = mu_j[:,i] + torch.sum(torch.matmul(logsig_i_minus_i.reshape(-1,1,self.z_dim - 1), \n",
    "#                                                           torch.linalg.inv(logsig_minus_minus)).squeeze()*diff_minus, axis = 1)\n",
    "#                 # absolute value for distance measure\n",
    "#                 dist_j += torch.abs(do_E - E)\n",
    "                \n",
    "#             Dist += torch.mean(dist_j)*prob_c[j]\n",
    "        \n",
    "#         ## MSE reconstruction loss\n",
    "#         reconstruction_error = F.mse_loss(output_list[0], x1) + F.mse_loss(output_list[1], x2)\n",
    "            \n",
    "#         loss = reconstruction_error + Dist\n",
    "            \n",
    "#         return loss, reconstruction_error, Dist\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8a433afd-6509-4ade-9b3f-b1119d9d5af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class doVAE_v2(nn.Module):\n",
    "\n",
    "    def __init__(self,z_dim, c_dim = 2, in_channels=3):\n",
    "        super(doVAE_v2, self).__init__()\n",
    "        \n",
    "        self.z_dim = z_dim\n",
    "        self.c_dim = c_dim\n",
    "        # encoder output dim: c_Num*z_dim for mu_z, c_Num*z_dim for sigma_z, c_Num for mu_pi, c_Num for sigma_mu\n",
    "        self.model_encoder= nn.Sequential(\n",
    "            # B,  32, 32, 32\n",
    "            nn.Conv2d(in_channels, out_channels=32, kernel_size=4, stride=2, padding=1),          \n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            # B,  32, 16, 16\n",
    "            nn.Conv2d(32, out_channels=32, kernel_size=4, stride=2, padding=1),  \n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            # B,  64,  8,  8\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1),  \n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            # B,  64,  4,  4\n",
    "            nn.Conv2d(64, 64, kernel_size=4, stride=2, padding=1),          \n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            # B, 256,  1,  1\n",
    "            nn.Conv2d(64, 256, kernel_size=4, stride=1),           \n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            View((-1, 256*1*1)),                 # B, 256\n",
    "            \n",
    "            # for mu and logvar\n",
    "            nn.Linear(256,  c_dim*(2*z_dim+2)),      \n",
    "        )\n",
    "        \n",
    "            \n",
    "        self.model_decoder = nn.Sequential(\n",
    "            nn.Linear(z_dim, 256),               # B, 256\n",
    "            View((-1, 256, 1, 1)),               # B, 256,  1,  1\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(256, 64, 4),      # B,  64,  4,  4\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 64, 4, 2, 1), # B,  64,  8,  8\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, 4, 2, 1), # B,  32, 16, 16\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 32, 4, 2, 1), # B,  32, 32, 32\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, in_channels, 4, 2, 1),  # B, 3, 64, 64\n",
    "        )\n",
    "        \n",
    "        hidden_dim = [32, 32]\n",
    "        # parametrize P(z|c) ~ N(mu_c_prior, I)\n",
    "        self.prior_layer= nn.Sequential(\n",
    "            # B,  32, 32, 32\n",
    "            nn.Linear(c_dim, hidden_dim[0]),          \n",
    "            nn.Tanh(),\n",
    "            # B,  32, 16, 16\n",
    "            nn.Linear(hidden_dim[0], hidden_dim[1]),  \n",
    "            nn.Tanh(),                 # B, 256\n",
    "            # for mu and logvar\n",
    "            nn.Linear(hidden_dim[1],  z_dim),      \n",
    "        )\n",
    "\n",
    "        self.weight_init()\n",
    "        \n",
    "    def weight_init(self):\n",
    "        for block in self._modules:\n",
    "            for m in self._modules[block]:\n",
    "                kaiming_init(m)\n",
    "        \n",
    "                \n",
    "    def encode(self, X):\n",
    "        \n",
    "        result = self.model_encoder(X)\n",
    "        X_len = X.shape[0]\n",
    "        mu = torch.zeros(X_len,self.c_dim * self.z_dim)\n",
    "        logvar = torch.zeros(X_len,self.c_dim * self.z_dim)\n",
    "        mu_pi = torch.zeros(X_len,self.c_dim)\n",
    "        var_pi = torch.zeros(X_len,self.c_dim)\n",
    "        for c in range(self.c_dim):\n",
    "            mu[:,c*self.z_dim:(c+1)*self.z_dim] = result[:,c*self.z_dim : (c+1)*self.z_dim]\n",
    "            logvar[:,c*self.z_dim:(c+1)*self.z_dim] = result[:,(self.c_dim+c)*self.z_dim : (self.c_dim+c+1)*self.z_dim]\n",
    "            mu_pi[:,c] = result[:,2*self.c_dim*self.z_dim + c]\n",
    "            var_pi[:,c] = result[:,2*self.c_dim*self.z_dim + self.c_dim + c]\n",
    "                                \n",
    "        \n",
    "        return mu, logvar, mu_pi, var_pi\n",
    "                                 \n",
    "    \n",
    "    def decode(self, z):\n",
    "        \n",
    "        return self.model_decoder(z)\n",
    "    \n",
    "    \n",
    "    def generate(self, num_samples, current_device):                         \n",
    "        z = torch.randn(num_samples, self.z_dim)\n",
    "        z = z.to(current_device)\n",
    "        samples = self.decode(z)\n",
    "        return samples\n",
    "    \n",
    "\n",
    "    def reparameterize(self, mu, logvar,mu_pi,var_pi,label):\n",
    "        X_len = mu.shape[0]\n",
    "        z_all = torch.zeros(X_len, self.z_dim)\n",
    "        mu_c = torch.zeros(X_len, self.z_dim)\n",
    "        log_var_c = torch.zeros(X_len, self.z_dim)\n",
    "        Pi = torch.zeros(X_len, self.c_dim)\n",
    "        for c in range(self.c_dim):\n",
    "            std = torch.exp(0.5 * logvar[:,c*self.z_dim:(c+1)*self.z_dim])\n",
    "            eps = torch.randn_like(std)\n",
    "            curr_z = eps * std + mu[:,c*self.z_dim:(c+1)*self.z_dim]\n",
    "            \n",
    "            std_pi = torch.exp(0.5 * var_pi[:,c])\n",
    "            eps_pi = torch.randn_like(std_pi)\n",
    "            curr_pi = eps_pi * std_pi + mu_pi[:,c]\n",
    "            \n",
    "            z_all += curr_z * curr_pi[:,None]\n",
    "            Pi[:,c] = curr_pi\n",
    "            # hard clustering for KL divergence\n",
    "            mu_c +=  mu[:,c*self.z_dim:(c+1)*self.z_dim] * label[:,c][:,None]\n",
    "            log_var_c += logvar[:,c*self.z_dim:(c+1)*self.z_dim]*label[:,c][:,None]\n",
    "        \n",
    "        return z_all, mu_c, log_var_c, Pi\n",
    "    \n",
    "    \n",
    "\n",
    "    def forward(self, X,label):\n",
    "        mu, logvar, mu_pi, var_pi = self.encode(X)\n",
    "        z, mu_c, log_var_c, Pi = self.reparameterize(mu, logvar, mu_pi, var_pi,label)\n",
    "        mu_c_prior = self.prior_layer(label.float())\n",
    "        reconstructed_z = self.decode(z)\n",
    "        return reconstructed_z, mu_c, log_var_c, mu_c_prior, Pi\n",
    "    \n",
    "    def calculate_loss(self, X, reconstructed_z, mu_c, log_var_c, mu_c_prior, Pi, label, pi_weight = 1.0, kl_weight = 0.5):\n",
    "        \"\"\"\n",
    "        Loss term currently contains: reconstruction loss, classification loss for p(c|x)\n",
    "        \"\"\"\n",
    "        # classification loss:\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        pi_loss = criterion(Pi, label.float())\n",
    "        \n",
    "        # reconstruction loss:\n",
    "        rec_loss = F.mse_loss(reconstructed_z, X)\n",
    "        \n",
    "        kld_loss = torch.mean(-0.5 * torch.sum(1 + log_var_c -(mu_c - mu_c_prior)**2 - log_var_c.exp(), dim = 1), dim = 0)\n",
    "        \n",
    "        loss = rec_loss + pi_weight * pi_loss + kl_weight * kld_loss\n",
    "        \n",
    "        return loss, rec_loss, pi_loss, kld_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6d81a6e9-19ad-43c2-9d23-6fdb62dbf67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4b0c64ab-f2b4-45f2-b70b-6642b157cf19",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'v1'\n",
    "if model_type == 'v1':\n",
    "    CD_model = doVAE(z_dim=4)\n",
    "elif model_type == 'v2':\n",
    "    CD_model = doVAE_v2(z_dim=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "5ec1511e-1f8c-4930-82e2-0a15a1e68dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = len(train_loader) * 128\n",
    "learning_rate = 0.01*float(128)/256.\n",
    "optimizer = torch.optim.SGD(CD_model.parameters(), lr=1e-3, \n",
    "                            momentum=0.9, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec1c0a3-c95e-49f8-9b8c-d806d367abc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                    | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][15/198]\tTime 0.195 (0.195)\tLoss 0.26293 (0.2629)\t\n",
      "Epoch: [0][30/198]\tTime 0.185 (0.185)\tLoss 0.26364 (0.2636)\t\n",
      "Epoch: [0][45/198]\tTime 0.196 (0.196)\tLoss 0.27257 (0.2726)\t\n",
      "Epoch: [0][60/198]\tTime 0.213 (0.213)\tLoss 0.25392 (0.2539)\t\n",
      "Epoch: [0][75/198]\tTime 0.200 (0.200)\tLoss 0.24980 (0.2498)\t\n",
      "Epoch: [0][90/198]\tTime 0.245 (0.245)\tLoss 0.26786 (0.2679)\t\n",
      "Epoch: [0][105/198]\tTime 0.189 (0.189)\tLoss 0.25181 (0.2518)\t\n",
      "Epoch: [0][120/198]\tTime 0.191 (0.191)\tLoss 0.24954 (0.2495)\t\n",
      "Epoch: [0][135/198]\tTime 0.203 (0.203)\tLoss 0.23914 (0.2391)\t\n",
      "Epoch: [0][150/198]\tTime 0.201 (0.201)\tLoss 0.25507 (0.2551)\t\n",
      "Epoch: [0][165/198]\tTime 0.199 (0.199)\tLoss 0.24802 (0.2480)\t\n",
      "Epoch: [0][180/198]\tTime 0.186 (0.186)\tLoss 0.25821 (0.2582)\t\n",
      "Epoch: [0][195/198]\tTime 0.188 (0.188)\tLoss 0.28102 (0.2810)\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|██▋                                     | 1/15 [18:09<4:14:08, 1089.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][15/198]\tTime 0.216 (0.216)\tLoss 0.26770 (0.2677)\t\n",
      "Epoch: [1][30/198]\tTime 0.252 (0.252)\tLoss 0.26538 (0.2654)\t\n",
      "Epoch: [1][45/198]\tTime 0.207 (0.207)\tLoss 0.25098 (0.2510)\t\n",
      "Epoch: [1][60/198]\tTime 0.206 (0.206)\tLoss 0.25533 (0.2553)\t\n",
      "Epoch: [1][75/198]\tTime 0.189 (0.189)\tLoss 0.24685 (0.2468)\t\n",
      "Epoch: [1][90/198]\tTime 0.188 (0.188)\tLoss 0.26011 (0.2601)\t\n",
      "Epoch: [1][105/198]\tTime 0.189 (0.189)\tLoss 0.25668 (0.2567)\t\n",
      "Epoch: [1][120/198]\tTime 0.184 (0.184)\tLoss 0.26756 (0.2676)\t\n"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(15)):\n",
    "    CD_model.train()\n",
    "    i = 0\n",
    "    for x, target in train_loader:\n",
    "        ### labels\n",
    "        enc = OneHotEncoder(handle_unknown='ignore')\n",
    "        enc.fit(target)\n",
    "        target = torch.from_numpy(enc.transform(target).toarray())\n",
    "        #####\n",
    "        \n",
    "        i += 1\n",
    "        batch_time = AverageMeter()\n",
    "        losses = AverageMeter()\n",
    "        end = time.time()\n",
    "        total_start = time.time()\n",
    "        \n",
    "        if model_type == 'v1':\n",
    "            reconstructed_z, mu, logvar, Pi = CD_model(x)\n",
    "            loss, rec_loss, pi_loss, kld_loss = CD_model.calculate_loss(x, reconstructed_z, mu, logvar, Pi, target)\n",
    "        if model_type == 'v2':\n",
    "            reconstructed_z, mu_c, log_var_c, mu_c_prior, Pi = CD_model(x,target)\n",
    "            loss, rec_loss, pi_loss, kld_loss = CD_model.calculate_loss(x, reconstructed_z, mu_c, log_var_c, mu_c_prior, Pi, target)\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i%15 == 0:\n",
    "        \n",
    "            reduced_loss = loss.data\n",
    "            losses.update(float(reduced_loss), x.size(0))\n",
    "            torch.cuda.synchronize()\n",
    "            batch_time.update((time.time() - end)/15)\n",
    "            end = time.time()\n",
    "\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                          'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                          'Loss {loss.val:.5f} ({loss.avg:.4f})\\t'.format(\n",
    "                           epoch, i, len(train_loader),\n",
    "                           batch_time=batch_time,\n",
    "                           loss=losses))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5c438c-436b-4742-8b10-79e4f0d26ab0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae80ca9f-b835-4fde-a7ff-fa71205355d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
